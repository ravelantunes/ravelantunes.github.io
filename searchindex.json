[{"section":"Blog","slug":"/blog/post-fragmented-enterprise-knowledge/","title":"The Hidden Risk in AI Products: Fragmented Enterprise Knowledge","description":"Enterprise AI fails without context management. Learn the risks of fragmented embeddings and how to build scalable, secure AI knowledge systems.","date":"July 22, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/fragmented-enterprise-knowledge/vGrk8Ks4lqH7LMOkD63jTg_hu17713876452840934136.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"236\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/fragmented-enterprise-knowledge\\/vGrk8Ks4lqH7LMOkD63jTg_hu17099715318823860808.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/fragmented-enterprise-knowledge/vGrk8Ks4lqH7LMOkD63jTg_hu262360985620136178.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/fragmented-enterprise-knowledge\\/vGrk8Ks4lqH7LMOkD63jTg_hu9862780435133679597.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Artificial Intelligence, AI Strategy, AI Governance, AI Architecture, AI Enterprise","tags":"AI","content":"The Hidden Risk in AI Products: Fragmented Enterprise Knowledge\nEnterprises everywhere are racing to adopt generative AI. From Microsoft 365 Copilot to ChatGPT Enterprise and beyond. These tools promise productivity, but quietly demand full access to your internal knowledge.\nThe takeaway: If leaders don\u0026rsquo;t influence how knowledge flows inside the enterprise, every AI tool will define it for them, and you\u0026rsquo;ll pay in risk, cost, and control.\nThe cost of doing nothing? Your data gets fragmented across tools you don\u0026rsquo;t control. Vendors gain leverage. Governance breaks. Engineering teams duplicate effort. And your AI becomes weaker, not smarter. Without a clear knowledge strategy, you\u0026rsquo;ll spend more, move slower, and lose control over the very context AI needs to work.\nStatus quo without strategy: each AI platform builds its own bridge to the same enterprise tools, resulting in redundant effort, fractured governance, and security headaches.The Hidden Pattern: Parallel Embedding Silos\nHere\u0026rsquo;s what\u0026rsquo;s happening behind the scenes:\nAI tools rely on indexing your internal content - SharePoint docs, emails, Confluence pages, CRM records - and converting them into embeddings, mathematical representations capturing the meaning of information. Each vendor does this separately, resulting in multiple parallel embedding silos across your enterprise. This seemingly harmless duplication creates real problems:\nRedundant ingestion: Multiple tools re-process identical content, wasting resources. Security exposure: Sensitive information ends up duplicated across numerous external databases. Inconsistency: Variations in freshness, scope, and versions of your knowledge emerge. Loss of control: You lose visibility into where your enterprise knowledge resides. Suddenly, the truth about your business isn\u0026rsquo;t just distributed, it\u0026rsquo;s fragmented.\nEmbeddings and Vector Search: Meaning over Keywords To truly appreciate why AI-driven search is such a big deal, let\u0026rsquo;s first demystify how it fundamentally differs from traditional search.\nTraditional search is straightforward: you type keywords, and the search engine matches those keywords directly with content. It\u0026rsquo;s like looking up terms in a book\u0026rsquo;s index. You find exact matches, but nothing deeper. AI-powered search, on the other hand, matches meaning.\nEmbeddings let AI understand the meaning behind content, not just the exact words used. They organize information based on similarity in meaning, so two pieces of content that say the same thing in different ways still end up connected. This is how AI finds what\u0026rsquo;s relevant even when the keywords don\u0026rsquo;t match.\nData needs to be converted into embeddings by a specialized embedding model to make it useful for generative AI\nWhen AI searches content, it first needs this map. To build it, the AI \u0026ldquo;catalogues\u0026rdquo; or indexes content by turning everything into embeddings ahead of time. Every document, email, webpage, or conversation that you\u0026rsquo;d want AI to consider as part of it is converted into these numerical representations. Later, when you ask a question, the AI converts your query into an embedding too, placing your question onto the same meaning map.\nHere\u0026rsquo;s where the magic (more accurately, math) comes into play: AI uses mathematical techniques like calculating vector distances to find the closest matches between your query embedding and the pre-cataloged content embeddings. By not relying directly on keywords, AI can recognize that two documents are related in meaning even if they don\u0026rsquo;t share any keywords, or are completely different modalities (audio, images, etc).\nThe query is converted into an embedding and used to find relevant documents on.\nHowever, this powerful technique comes with complexities: The AI must catalog all potentially useful content in advance. This indexing is a heavy, ongoing process, as content continuously updates.\nEmbeddings are model-specific. GPT, Gemini, Claude, and others each generate embeddings differently, and even model updates can shift that behavior. This specificity directly impacts how well AI understands your content. Critically, your query embedding must match the embedding model used for cataloging. If your content was catalogued using Gemini, GPT models wouldn\u0026rsquo;t correctly interpret its embeddings, causing retrieval failures or inaccuracies.\nThe Data Challenge: Why Indexing is Harder Than it Sounds AI search requires upfront, consistent cataloging of your data into embeddings using a specific AI model. Every email, document, SharePoint file, or wiki page must be translated into vectors, even though these systems were never designed for AI consumption. Since embeddings are model-specific, queries must use the same embedding model used for indexing. This technical detail creates a strategic risk: every vendor is building its own meaning map, leading to fragmented silos and unreliable enterprise search.\nTraditional enterprise data warehouses were optimized for human-driven reporting and analytics, summarizing and trimming data for easier consumption by people. However, this trimming removes the nuanced context and detailed meaning that AI relies on to be effective. In other words, data prepared for human eyes might be insufficient or even misleading for AI embedding.\nAdditionally, critical day-to-day data like emails, instant messages, and collaborative documents have rarely been considered strategic assets. They now form the backbone of many powerful AI-driven workflows. Yet, current AI models require extremely broad, nearly god-level access to your enterprise\u0026rsquo;s entire data landscape to catalog it effectively.\nFor example, for Microsoft 365 Copilot admins must grant application-level consent to access files and email data, not just individual user accounts. Google Gemini Enterprise similarly needs enterprise permissions.\nEach AI vendor wants to independently perform this cataloguing and embedding, meaning that multiple external providers each hold a full copy of your internal knowledge.\nFurther complicating matters, these embeddings aren\u0026rsquo;t a one-time process. Whenever new content arrives or existing content changes - think daily emails, updated documents, or collaborative edits - the embedding catalogue must be continuously refreshed. Managing this ongoing ingestion across multiple siloed tools creates significant overhead, security risks, and governance challenges.\nThe Governance and Permissioning Minefield Granting AI tools comprehensive, god-level access to your internal systems isn\u0026rsquo;t just technically demanding - it creates profound governance challenges. Every piece of enterprise data has unique access permissions, often highly nuanced and complex, based on user roles, team memberships, and organizational hierarchies.\nFor example, permissions in systems like Google Drive or SharePoint can vary down to individual files and folders. Replicating these permissions accurately across external AI embedding stores is both technically complex and prone to error. Currently, most AI systems either try to mirror these permissions asynchronously and imperfectly or perform permission checks at query time - again, often imperfectly.\nReal-world incidents illustrate how significant this issue is. Microsoft 365 Copilot and Slack\u0026rsquo;s recent AI prompt injection vulnerability demonstrate that current AI systems often inadvertently expose sensitive information due to poorly replicated access controls or flawed permission logic. These incidents typically result from:\nEmbedding sensitive data that should have been excluded. Out-of-sync or overly broad permissions logic. Vulnerabilities in the AI prompt logic that trick the system into exposing confidential data. There is no robust, standardized approach today for AI tools to enforce access controls consistently across federated systems like SharePoint, Drive, or Slack. Each vendor maintains its embedding store, attempting to mirror permissions and enforce ACLs, resulting in varying degrees of success and significant security risks.\nUltimately, this isn\u0026rsquo;t merely a series of isolated bugs - it\u0026rsquo;s a fundamental design flaw in how AI systems handle enterprise knowledge today. Enterprises must address these architectural gaps proactively to safely harness AI\u0026rsquo;s full potential.\nA Practical Framework for Building Your Knowledge Strategy To avoid the fragmentation and governance traps of today\u0026rsquo;s AI tools, enterprises must take ownership of their knowledge layer. Here\u0026rsquo;s a practical framework for those leading AI, product, or data strategy to guide their approach:\nInventory: Catalog all knowledge assets (emails, docs, tickets, contracts, chats, etc.), including where they live, who owns them, and how frequently they change. Ownership: Define a clear ownership model. Will data domains be managed by distributed teams (a la data mesh)? Or will a centralized knowledge platform team maintain the core pipelines, versioning, and semantic models? Ownership clarity prevents entropy Integration: Many internal systems (especially legacy ones) won\u0026rsquo;t fit cleanly into modern paradigms. Create adapters, wrappers, or APIs that make them searchable and safe to expose to AI tools. If you don\u0026rsquo;t build the bridge, vendors will build and own the interface. Infrastructure: Decide how AI agents and copilots will access knowledge: centralized store, federated protocol (like MCP), or hybrid. Build consistent interfaces and ensure permission logic travels with the query, not just the data. Govern: Inject Responsible AI gates early. Bake in observability, auditability, ACL enforcement, and red-teaming at the retrieval layer. Security, fairness, and privacy must be native, not bolted on. Navigating this high-stakes, rapidly evolving landscape requires more than adopting the latest AI tool, but having architectural foresight. Enterprise vendors are racing to position themselves at the center of the AI ecosystem, and control over internal knowledge is becoming a key strategic lever. Platforms like Microsoft SharePoint, Google Workspace, and Salesforce are unlikely to fully embrace open integration in the near term. If a vendor can become the default gateway to an organization\u0026rsquo;s internal knowledge, they hold disproportionate power over retrieval, which copilots, agents, or workflows can thrive. Opening up too soon means losing that competitive edge.\nAnd yet, no single platform can deliver a complete AI experience in isolation. Most organizations rely on a constellation of tools, each housing fragments of institutional knowledge. As a result, enterprises will need to think carefully about how they expose and retrieve this knowledge in a way that balances interoperability, security, and control. While no standard architecture has emerged, most paths forward will likely resemble one of a few foundational patterns, each with distinct tradeoffs and long-term implications.\nCentralized Knowledge Store Centralized embeddings, federated flexibility: this architecture puts the enterprise back in charge. Index once, use anywhere.\nA centralized embedding store that aggregates all your enterprise data in a tool that is owned and managed by the enterprise, in addition to an embedding model that third-party tools can use to interact with the embedding data.\nAdvantages Complete control and governance over indexing, versioning, permissions, and storage. Reduced vendor lock-in and greater flexibility to experiment with new AI tools safely. Minimized risk, as you avoid giving external vendors unrestricted access. Disadvantages Requires substantial initial investment and potentially high ongoing costs. Vendors may resist adopting this architecture until market pressures force their hand. Managed services from hyperscalers (AWS, Google, etc.) using open protocols could eventually alleviate vendor lock-in. Implementation The enterprise stands up a central embedding pipeline: data from email, docs, chat, ERP, CRM, etc. is continuously ingested, chunked, embedded, and stored in a vector database. This database is enriched with metadata and ACLs (user, department, clearance level), allowing permission-aware search. The embedding model (e.g., OpenAI Ada, Cohere, or internal) is exposed via an API for query embedding, so tools like copilots or agents can embed the user\u0026rsquo;s prompt in the same space as the content. External tools don\u0026rsquo;t index content themselves; they submit semantic queries to the centralized API (embedding + search call). ACL rules from upstream systems are maintained up-to-date in real-time, allowing query-time ACL enforcement. Fully Federated Approach (MCP or similar protocol) Standardized routing over centralization: a federated approach where systems remain the system of record, and AI interfaces query through a unified abstraction layer.\nAnother path involves adopting a standardized an API-driven protocol like MCP to act as a federated integration layer, allowing systems to expose their data securely while enforcing native permission controls.\nAdvantages Facilitates bridging legacy or other systems not yet adapted to AI, as protocols like MCP make it easy to wrap existing APIs within MCP. Simplified integration when leveraging protocols that are becoming industry standard, like MCP. It can leverage existing resource authorization logic already in place in existing APIs. Disadvantages Search capabilities may be limited, as queries typically remain in plain-text, with the source system controlling embedding and search algorithms. Advanced AI search techniques available to dedicated embedding stores might not be feasible in this federated model.\nImplementation Each system (Salesforce, Jira, Workday, etc) implements an MCP-compatible server allowing raw text queries or specific data requests. This request will include a token that represents the user, which can be used to authorize the resources that the user has access to and filter with the built-in ACL logic. The embedding, chunking, and retrieval logic stay inside the source system, rather than being centralized. The calling AI agent never accesses raw documents - only what\u0026rsquo;s authorized via the exposed API. Conclusion While centralized and MCP-style architectures offer clear starting points, most enterprises will likely evolve toward some form of hybrid model. Some systems may expose their own embedding stores and models via APIs, enabling external tools to retrieve semantically indexed content directly - a \u0026ldquo;bring-your-own-retriever\u0026rdquo; approach. This can unlock domain-specific optimization, while maintaining some decoupling.\nWe\u0026rsquo;re still early in the enterprise AI journey. The landscape is moving fast: tools, protocols, models, and standards are all in flux. This is not the time to rigidly commit to a single architecture. Instead, design for flexibility and adaptability. We don\u0026rsquo;t yet know how the AI-native enterprise stack will settle, much like no one could have sensibly planned a five-year e-commerce strategy in 1997.\nBut just because we can\u0026rsquo;t predict everything doesn\u0026rsquo;t mean we should build blindly. Some architectural truths are already clear and worth designing for today:\nEmbedding infrastructure is foundational. Even if you federate access, you\u0026rsquo;ll almost certainly need internal embedding pipelines to support internal development and use cases. Consider this an inevitable cost center, one that pays dividends when approached strategically. Communication data will require its own strategy. Email, chat, and messaging data introduce unique challenges: they\u0026rsquo;re highly sensitive, context-specific, and often user-specific in terms of what should be retrieved and by whom. - Traditional data governance patterns don\u0026rsquo;t apply cleanly here. You\u0026rsquo;ll need new approaches that respect personal boundaries, enforce strict access controls, and design retrieval to be aware of social context Platform decoupling will remain critical. AI tools, embedding infrastructure, and storage systems should evolve independently. Avoid tightly coupling retrieval logic to any one vendor. Governance and security require upfront design. Access control, observability, and compliance shouldn\u0026rsquo;t be patched in later. They must be native to your retrieval architecture from day one. The most resilient organizations will be those that invest in adaptable architecture today, respecting what\u0026rsquo;s to change, but building intelligently around what won\u0026rsquo;t.\n"},{"section":"Blog","slug":"/blog/post-stop-make-ai-look-human/","title":"Why We Should Stop Making AI Look Human","description":"Exploring why AI interfaces shouldn’t try to replicate human appearance and instead focus on functional, efficient design.","date":"July 13, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/stop-make-ai-look-human/6Bl1zO6zJddN9umNV2ftKQ_hu5178856922265002371.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/stop-make-ai-look-human\\/6Bl1zO6zJddN9umNV2ftKQ_hu5178856922265002371.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/stop-make-ai-look-human/6Bl1zO6zJddN9umNV2ftKQ_hu11712645889240149323.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/stop-make-ai-look-human\\/6Bl1zO6zJddN9umNV2ftKQ_hu11712645889240149323.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Artificial Intelligence","tags":"AI, User Experience, Design","content":"Making AI visually indistinguishable from humans might seem compelling, but it’s a misguided goal that introduces friction, unmet expectations, and poor design.\nAs I write this (mid-2025), AI has rapidly transitioned from behind-the-scenes systems to daily interactive and conversational engagements. Text and chat remain dominant modes — straightforward, effective, and essential for everything from coding to email management.\nAudio interfaces are also evolving: so far, it has been used for very transactional commands (Alexa, Siri, etc.), but we’re starting to see more nuanced interactions with voice assistants that can handle complex AI conversations (ChatGPT voice chat).\nAnd now, we’re on the verge of another shift: AI interfaces that mimic human appearance and behavior.\nAt first glance, human-like interfaces feel natural and advanced. But do they improve how we interact with AI, or do they create confusion, distraction, and a false sense of capability that the tech can’t deliver?\nVisual Anthropomorphization: An Unnecessary Frontier It’s easy to imagine that the next wave of AI interfaces will include avatars that will be visually indistinguishable from humans.\nImagine a conversational AI represented by a lifelike avatar, complete with realistic expressions, subtle eye movements, and natural gestures. It could smile warmly to express agreement, furrow its brows slightly in confusion, or nod encouragingly during conversation. The avatar’s eyes might track your movement or gaze direction, reacting with appropriate emotional responses, such as empathy, surprise, or attentiveness. Essentially, interacting with it would visually feel like speaking directly with another human being.\nMovies and futuristic narratives often present AI this way. The allure is understandable: familiar faces and humanized interactions feel natural, relatable, and advanced. But do we need AI to replicate human interaction, or are we chasing a mirage of optimal communication?\nDesigning AI to be indistinguishable from humans is a path fraught with distractions, unmet expectations, and misplaced effort. We should aim for interaction methods uniquely suited to AI and digital mediums.\nInterface Evolution Proves Simpler is Better The evolution of interface design has already taught us valuable lessons. I consider the example of the transition from skeuomorphic to flat design on computers and phones, which highlighted that focusing only on essential visual signals greatly improves usability.\nSkeuomorphic design was the norm in early computing and was very prominent in the early days of mobile apps. Think of the first versions of iOS. It included plenty of shadows, textures, gradients, lighting effects, 3D buttons, and other visual elements that mimicked real-world objects. The idea was to make digital interfaces feel familiar and intuitive by replicating physical objects like books, buttons, and switches.\nFlat design removed unnecessary detail, reducing cognitive load and confusion. Similarly, multi-touch interfaces on the iPhone succeeded by inventing gestures explicitly optimized for fingers, and not by mimicking older desktop paradigms. In a way, it proposed the removal of realism to create a more efficient human-computer interaction.\nThese lessons strongly suggest AI interfaces should evolve by identifying their strengths and limitations, rather than trying to replicate human interaction.\nDesigning AI interfaces to look and act human introduces a core mismatch between appearance and capability. Realistic avatars raise user expectations of empathy, context awareness, or human-like reasoning that current AI can’t meet. This creates friction: small lapses become jarring, interactions get bogged down by unnecessary social cues, and trust erodes in the uncanny space between “almost human” and “clearly not.” Instead of enhancing the experience, realism often distracts from it.\nConsider “Emma”, which the City of Amarillo added to its website to help residents find information. As a test, I once asked her, “What are trash pickup days?”. The answer was vague and unhelpful: “It happens during weekdays but can change depending on holidays. For more information, you can call the waste management company.” This completely missed the mark. A lifelike AI face creates the expectation that it has immediate access to local knowledge, or at the very least, the awareness to ask for my address or neighborhood and retrieve a specific answer. Instead, the interaction felt shallow and frustrating. The realism set a bar that the system couldn’t meet, and the result was a worse experience than a simple, well-designed FAQ page.\nVisual Cues, Not Visual Realism While full realism has clear drawbacks, visual communication itself remains vital. Simple visual signals can convey AI’s status (listening, processing, confusion, agreement) and significantly streamline interactions. In some cases, expressive characters that can convey emotions or reactions can enhance engagement without mimicking human complexity.\nHere are some examples, which can be thought of as a spectrum — from purely functional cues to more expressive visual personas — that enhance interaction without crossing into human mimicry:\nFunctional status indicators: Loading spinners, color-coded feedback, subtle animations. These are purely functional visuals, clearly indicating system states without attempting to convey any personality. Animated icons with persona: A good example is Siri’s colorful waveform. Simple visuals are designed to subtly convey a sense of presence or persona, making interactions feel slightly more personal, yet non-human. Expressive non-human characters: Think Wall-E’s eye expressions, R2-D2’s beeps and head tilts, or Duolingo’s playful owl. These characters succeed because they signal intent, emotion, and presence clearly, without ever pretending to be human. They avoid the uncanny valley entirely while still engaging users in emotionally resonant ways. This design choice builds trust and personality without overpromising intelligence or empathy. If realism isn’t the right path forward, what should guide effective AI interface design? Here are four core design principles that put usability and trust first:\n1. Authenticity The interface should never pretend the AI is more capable than it is. Avoid visual cues that imply human-level perception, empathy, or intelligence unless those capabilities exist.\nExample: An avatar that maintains eye contact should be backed by actual camera input and gaze tracking, or it risks creating a false sense of awareness and trust.\n2. Efficiency Convey status or emotion using the simplest visual or interaction method available. Don’t replicate human dialogue if a click or tap is faster.\nExample: Filling out a shipping address is easier through a form than by speaking it line by line.\n3. Contextual Appropriateness Match the interface design to the task and setting. Use minimal, efficient visuals for transactional tasks, and richer, more expressive ones for emotional or immersive experiences. Speed isn’t always the priority: some interactions benefit more from tone, presence, or emotional resonance.\nExample: A children’s storytime app may benefit from a warm, animated character. But a smart home AI is better served by simple visual confirmations or audio cues that don’t distract from the task.\n4. Accessibility and User Control Allow diverse users to interact comfortably and maintain control over their experience and preferred method of interaction modes or interface complexity.\nExample: Some people prefer typing over speaking; others love using voice input. A good AI interface doesn’t force one method — it gives users control over how they interact, whether through text, voice, or visual prompts.\nThe best AI interfaces won’t fool us into thinking they’re human. They’ll help us become more human by clearly communicating through minimal yet expressive signals, complementing our strengths rather than mimicking them.\n"},{"section":"Blog","slug":"/blog/post-humans-as-agents/","title":"Building Agentic Systems: Why You Should Start with Humans as AI Agents","description":"Learn why starting with &#39;Humans as AI Agents&#39; can accelerate safe, scalable agentic AI development. A framework for responsible automation, from manual workflows to full autonomy.","date":"April 26, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/humans-as-agents/human-as-agents_hu10373540313658418336.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"356\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/humans-as-agents\\/human-as-agents_hu10373540313658418336.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/humans-as-agents/human-as-agents_hu11070426364374228713.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/humans-as-agents\\/human-as-agents_hu11070426364374228713.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Artificial Intelligence","tags":"AI Agents, AI","content":"The rise of Agentic Systems is reshaping how we think about building complex AI solutions. Like early software architecture, the new wave focuses on modularity, separation of concerns, and safe boundaries — smaller specialized models working together, orchestrated by a central host system.\nIn theory, it’s clean. In practice, it’s messy.\nTo bring these systems to life, every agent, tool, and integration must operate at a level that’s acceptable in risk, correctness, and cost. Complexity doesn’t just add up, it compounds as you add additional agents, especially for tasks that demand human-like reasoning, judgment, and decision-making. Without a thoughtful approach, it’s easy to end up with brittle systems, unfinished proof-of-concepts, and automation that erodes trust instead of building it.\nThat’s why I believe the best way to build Agentic Systems today is to design for a fully autonomous future, but start with humans role-playing as AI agents for the riskiest and most critical tasks. Let the system mature step-by-step, instead of betting everything on day-one automation.\nWhat Does \u0026lsquo;Humans-as-Agents\u0026rsquo; Means The idea of humans-as-agents is to have humans performing the tasks that we eventually want to automate and be operated by AI, in the specific way we expect AI to eventually approach the task. This can be accomplished by having an integration with a tool like Slack, email, or custom-build app acting as a proxy to communicate with the human agent.\nWhy is this different than what we are doing today, if perhaps we already have those tasks being performed by humans? At least with today\u0026rsquo;s technology, the way we approach a process and break down the tasks for a system that is handled by humans is different than the way it would approach when handled by AI.\nHumans can perform complex tasks that are fluid, cross-domain, and more context-aware than AI systems.\nAI-systems tasks tend to be more bounded, explicitly defined, and focused on a specific domain or task.\nTake this example of a task to respond to a customer support request.\nHuman: Skim entire request, instantly spot name, tone, urgency Recall prior interactions \u0026amp; purchases from memory or CRM quick search Decide whether to resolve solo or loop in a colleague or superior, perhaps even consider exceptions to the rules Draft response mixing policy + personal judgment Hit send or escalate further AI System: Agent 1: Extract customer information from request Agent 2: Look up customer history on multiple databases to build context around customer Agent 3: Classify request type (complaint, question, feedback, etc) and assign to the right specialized agent Agent 4 (Specialized Agent for request type): Review knowledge base and guidelines on how to process response Agent 5: Generate response based on the information provided by the previous agents based on customer tone and context Agent 6: Validate response and send to customer, or generate an escalation if the confidence of resolution level is low. Why Start Here? Responsibility by Default: When you start with humans acting as agents, you start with the assumption of zero trust on AI, and gradually build up the trust in the system as it matures. Agile Development: You can start testing your system solution even if you don\u0026rsquo;t have all agents available. Process Discovery: as the human agent is performing the task, it provides a unique opportunity to develop an understanding of the task, challenges, and edge cases that might be encountered. Data Generation for Supervised Learning: as a human agent is performing the task, the system can collect data on how the human is performing the task, generating a trove of data that can be used to train and validate the future AI agent. Change Management: Build trust and cultural buy-in with early and direct participation of stakeholders in the process. It keeps them engaged in the development of the process, and can create a sense of ownership and accountability. AI Agent Maturity Stage To achieve that, I classify the following different stages of maturity to consider when building an Agentic System:\nStage 0 - No Agent: Interface is designed, but mocked up with predefined responses. Stage 1 - Manual Mode: the interface is used to interact with a human through a proxy communication channel (e.g., Slack, email, etc). Stage 2 - Copilot Mode: The model suggests partial outputs and a human assembles, edits, and signs off. At least one manual action is required so nothing ships without human eyes.1 Stage 3 - Human-in-the-Loop Mode: the model is now responsible to handle the task, but there\u0026rsquo;s still a human-in-the loop accountable to review the output and provide feedback to the system. Stage 4 - Human Escalation Mode: the AI system has reached a level of maturity where it can be trusted for the subset of the tasks without human input or review, but still not able to handle all of the tasks. The system is configured with a subsystem to assess the risk of the task or the confidence level of the model output, and based on those parameters or rules, can delegate this task to a human. Stage 5 - Fully Autonomous Mode: the AI system is fully autonomous and can be assumed to be able to handle all tasks. At this point, inability to handle a task correctly is considered a regression of the system. The system might still sample some tasks to a human to ensure that the system is continuously performing as expected, and leveraging this feedback to improve the model and prevent drift over time. Additional Thoughts and Considerations Building these systems isn’t just about orchestrating tasks — it’s about continuously measuring, improving, and adapting both agents and the system as a whole.\nHuman-in-the-Loop by Default: A side effect of starting with humans-as-agents is that the system is natively designed to keep a human-in-the-loop when needed — whether to increase oversight, pause or disable an agent or tool, or create additional labeled datasets. This can be used to set up regular re-labeling or validation rounds to perform quality checks and generate fresh supervised training data to control for drift. Quantify Performance Early: Build a system to track agent and system-level performance from the start. For some tasks, automated metrics (accuracy, latency, success rate) are enough. For others, especially more subjective outputs, human evaluation loops will be necessary to measure quality and correctness. Bias Management: Be cautious of bias leakage. If the human agents you use early on are not diverse, the AI systems will inherit those patterns. Consider strategies such as rotating human agents, diverse reviewer pools, or bias detection audits to mitigate this risk. Latency, Queueing, and Escalation: Consider whether you might face bottlenecks. Build some classifiers or scorers first can help with the routing and prioritization of tasks. For example: classifying requests that you can\u0026rsquo;t trust an AI agent yet and want to escalate to a human, while a priority scoring model can help you prioritize your agent\u0026rsquo;s tasks queue. Promotion Criteria: Define clear, measurable criteria for agents to graduate from one maturity stage to the next. For example: Sustained performance above a target metric (e.g., 98% match rate over 30 days) Human override rate below a threshold (e.g., \u0026lt;1%) Agentic Systems offer immense potential, but realizing that potential responsibly requires careful design, human insight, and progressive trust-building. Starting with humans-as-agents isn’t a compromise, it’s an operational blueprint for building robust, trustworthy AI ecosystems that can scale.\nPerhaps intentionally creating a UI that adds some friction to force additional human input to ensure that the human is still deeply engaged in the review.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"section":"Blog","slug":"/blog/post-resume-tips/","title":"Resume &amp; Interview Tips: A Tech Hiring Manager’s Perspective","description":"Boost your chances of landing a tech job with insider tips from a hiring manager. Learn how to perfect your resume, master the interview, and stand out to recruiters.","date":"April 11, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/resume/vitaly-gariev-qvbJkpIKotk-unsplash_hu11971902795533708574.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"236\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/resume\\/vitaly-gariev-qvbJkpIKotk-unsplash_hu11971902795533708574.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/resume/vitaly-gariev-qvbJkpIKotk-unsplash_hu10140843973108787484.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/resume\\/vitaly-gariev-qvbJkpIKotk-unsplash_hu10140843973108787484.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Hiring","tags":"job search, hiring","content":"I’m often asked to help with resume feedback and interview tips, and it has become even more frequent now that the tech market is tightening. I’ve implemented tech hiring processes across multiple teams and companies and usually am deeply involved in candidate screenings, so this post consolidates my thoughts and perspectives, as well as some \u0026ldquo;behind the scenes\u0026rdquo; into how the hiring process works.\nJob Posting and Resume Screening Before a job is posted, a company recruiter gets assigned to support the specific role. The hiring manager will provide the job description to the recruiter, and they will meet so the hiring manager can provide information about the role such as the team, tech stack, projects, and the type of candidate they are looking for. Sometimes this is a technical recruiter with experience hiring for tech roles, and sometimes it’s a recruiter that hires for multiple disciplines and not directly specialized in tech. This is the first person you will get to interact with.\nAs applications start to come in, it’s common that only the recruiter will receive notifications so they can screen and filter out as much noise as possible. The goal here is to do a \u0026ldquo;smell test\u0026rdquo; and filter out resumes that are clearly not a fit for the role, and pick which ones will get scheduled for a phone screening with the recruiter.\nIf the recruiter is not specialized in tech, or still getting familiar with the role or space, they might ask the hiring manager to help with this screening, but over time the recruiter tends to do this initial screening on their own. The recruiter will usually filter based on the their knowledge of what the hiring manager provided, look for specific keywords, and ensure that the resume meets some hard requirements like years of experience, education, and location.\nReasons you might get filtered out at this stage: Your resume doesn\u0026rsquo;t match the job description, either because you are really not a fit, or you are not conveying the right information so that a recruiter can understand. Your resume might fit the job description, but if the volume of qualified applications is high, the recruiter is prioritizing stronger resumes to send through next steps. There are other strong candidates reaching later stages of the interview process, someone already has an offer pending, or going through background check, so they are not looking to add more candidates to the pipeline even though they left the role open in case those other candidates fall through. Sometimes the hiring manager already has someone in mind, posting online and interviewing is just a formality to comply with HR policies (I know it\u0026rsquo;s lame). This is more common in larger companies. Tips: Fine-tune your resume to the specific job requirements. Make it easy for someone quickly scanning your resume to get a sense that you are a fit for this role. This might sometimes mean removing things that are not relevant. Be pro-active on potential red flags. For example, if you have gaps, try to fill it with some note or explanation. Otherwise, you are leaving room for interpretation. Be honest on your resume. I understand the urge to make it look as impressive as possible, but note that most companies will perform a background check, job history, and education check, usually after you have received the offer (which will be noted as “contingent on passing background checks”). Those verifications are commonly performed by a third-party, and their goal is to try to collect enough evidence of your background matching what you’ve put on the resume. They might ask letter of verification from former employers, or even ask for W-2’s for every year you worked if the company isn’t around anymore. I’ve seen instances of offers being rescinded due to titles being misrepresented, or unfinished degrees listed as completed, etc. The consequences here are not only losing the job, but also burning bridges with the company. Recruiter Phone Screening The recruiter will then reach out to schedule a phone screening. This is usually 15-30 minutes long. They will talk about the company, the team, the role, maybe go into benefits and perks, and then they will ask you a few questions, which can range from \u0026ldquo;tell me about yourself\u0026rdquo; to \u0026ldquo;what are your salary expectations\u0026rdquo;, but they can also get into some skills related questions, which might have been provided by the hiring manager to help with the screening. Here is important to note that the recruiter will ask you questions that will sound technical but chances are they are clueless about what you will be talking about. They are just asking questions provided by the hiring manager and they might have difficulty understanding all names and terms you mentioned. For example, \u0026ldquo;describe the list of AWS services you are most proficient with\u0026rdquo;. If you list a bunch of letter soup acronym like \u0026ldquo;EC2, S3, RDS, etc\u0026rdquo; the recruiter might not know what those are, while trying to keep up with transcribing your answer. I recommend keeping this in mind and trying to answer technical questions as if you are talking to a non-technical person: talk slowly, provide context (ie.: \u0026ldquo;compute services like EC2 and ECS, storage services like S3\u0026rdquo;) to ensure less things get lost in translation to be shared to the hiring manager later.\nAfter this initial call, if you are able to pass the recruiter criteria and not get filtered out at this stage, the recruiter will share your resume, as well as your answers from this initial screening, to the hiring manager.\nReasons you might get filtered out at this stage: Your non-technical answers could indicate a lack of fit for the company or team culture, or if skills questions were asked you might not have provided enough depth or context to demonstrate your knowledge. Same as before, other strong candidates might be progressing through the pipeline so they will hold off on moving new interviews to next step. Tips: Be prepared to answer questions about your resume, and be ready to provide examples of your work. This is a good time to ask questions about the role, the team, and the company. Be mindful on how to answer technical questions to the recruiter, as noted above. You might be asked salary expectations. This is a tricky question and I recommend doing some research on how to answer it in advance (this is big topic on it\u0026rsquo;s own and there\u0026rsquo;s a lot of existent advice on this topic so I won\u0026rsquo;t dive much into it). My tip here is mostly don\u0026rsquo;t get caught off guard and providing an answer that will handicap you later on. Think on how you will answer it prior to getting on the call. Ask questions about the process to the recruiter, including what the hiring manager is really looking for, and what to expect on next interviews. The next steps of the interviews are not meant to be a \u0026ldquo;Gotcha!\u0026rdquo; moment so if you can get some insight on what the hiring manager is looking for you can better prepare. I\u0026rsquo;ve been happy to provide additional information to candidates that asked me about the process. If you get some additional insight about the role that you think your resume doesn\u0026rsquo;t convey it, you can ask the recruiter if you can update your resume, which can help you being prioritized on the next steps. Hiring Manager Resume Review When the recruiter shares your resume and questions with the hiring manager most of the time your resume is shared in a batch of resumes, maybe anywhere between 5 or 10. The hiring manager will then review resumes with more attention to let the recruiter know the set of candidates that the hiring manager wants to proceed with in the interview. I’d say that at this point that only about 20% of the resumes end up getting scheduled for an interview. The hiring manager will provide the list of people to the recruiter to schedule screening calls, sometimes rank by priority.\nThere\u0026rsquo;s a limited bandwidth that a team can dedicate to interviews, since team members are juggling their time between projects and interviews. If there\u0026rsquo;s a high volume of candidates the interviews might end up queued, where the hiring manager will decide \u0026ldquo;let\u0026rsquo;s schedule interviews with A, B, and C first, and then we can look at D, E, and F\u0026rdquo;. However, there\u0026rsquo;s no guarantee that D, E, and F will ever get an interview. While they were queued, the hiring manager might have received another batch of resumes, and they might be stronger than D, E, and F, or they might be really confident on an ongoing candidate, so those can start falling out of priority.\nWhen I\u0026rsquo;m reviewing those batches of resumes, I’d say I spend less than a minute on each resume at this phase: most resumes I’ll scan very quickly for things I’m looking for, or lack of, or red flags. For example, one of the latest roles I was hiring for was a Director of MLOps. I would scan the resume to quickly answer the following:\ndo they have experience with ML frameworks? have they managed teams and other leaders before? does this person has experience on our cloud provider? If not, do they at least have strong experience on another cloud provider? does it seems like they managed platforms before? Or juggled multiple projects at once? Is a minute really enough to review a resume? Not for a full hiring decision, but the goal here is just to looking for making a decision on whether this person shows enough to move to the next step. I will thoroughly review resumes right before the interview.\nTips Like mentioned above, resumes are scanned quickly on this phase, so make sure to highlight the most relevant information. Use bold, italics, and bullet points to make it easy to read.\nKeep each job history on the resume concise. I’ve seen a lot of resumes that list a bunch of projects that the person been involved on. Not only it becomes a wall of text, but considering that projects are a team effort, my goal is to understand your real role and contribution. Did you make a real difference on that project? Or you \u0026ldquo;just happened to be there\u0026rdquo;? Focus on trying to convey your real contribution, even if less impressive than the project itself. If you really want to throw the kitchen sink on the resume in case there\u0026rsquo;s some keyword that might catch the recruiter eye, I recommend having a separate section for \u0026ldquo;Additional Projects\u0026rdquo; so you can list everything else, while keeping the job history focused on your core contributions.\nThings that Impress Me\nCode repo with open source projects: contribution to established open-source projects is a really strong signal to me. Most established projects have great technical standards so if you are getting merges on those projects it demonstrates you can fit that bar, that you can navigate guidelines, and has collaboration skills. I also put a lot of value on personal projects, even if not large, but that is actively iterated or maintained. It conveys to me passion for the craft and ownership. Thoughtful blogs: Blog posts that aim to share new knowledge or insightful point of views. An example here is Matt Dupree Philosophical Hacker. As a contracts, a post on \u0026ldquo;how to setup a Kubernetes cluster on AWS\u0026rdquo; with a bunch of copy-pasted commands and very little explanation or improvement on existing documentation is not very impressive. Job experience at companies that has strict hiring processes (ie.: FAANG): those companies itself have higher bars for hiring, so someone that has passed that bar once can be a strong signal. I would generalize that as well to elite schools with strong technical programs. Things That Don’t Impress Me\nCertifications: this rarely moves the needle, with some rare exceptions (ie.: AWS Professional Architect, which has a tough exam, and hard to pass with just superficial knowledge, online course, and no real experience), specially for more senior roles. This is because certifications alone rarely prove that your skills translates to real-world experience, which is what I’m looking for most of the time. If you can demonstrate real-world experience through your job history, then certifications at that point are moot. Repos with exercises from classes or online courses: it’s really hard to tell on those repos how much of the code you wrote, was handed out, etc. The simple completion of those exercises rarely implies knowledge or mastery in the subject. For more junior roles I might use some of the exercises as a conversation starter to see how they approached the problem (for more senior roles I’d likely use professional experience only instead). Word salad of languages and skills: Resumes usually list languages and tools that a person claim experience on. The challenge with those is that I realize people will throw anything there, even if they only touched it once in their life, so it’s hard to separate what is something the person is proficient or not. NOTE: I still recommend having this on resume as it can help prevent you from early filtering by tech recruiter based on keywords alone. Technical Screenings Those are either some code challenges or system design interviews administered by the technical folks on the team. Coding interview and system designs are not a perfect representation of the day-to-day work, specially with the peer pressure of an interview, but it\u0026rsquo;s still one of the best approach that companies have to assess a candidate\u0026rsquo;s skills.\nDepending on the role and company, this might be one or two interviews, or you might have series of technical interviews with different groups. For the latter one, before you are sent into it, you might do one \u0026ldquo;lightweight\u0026rdquo; technical interview with a single person. This is an early filtering to make sure you at least know how to code or have a basic foundation before sending you through a battery of coding and system design interviews. I won\u0026rsquo;t dive too deep into the technical interviews because this is usually the type of advice that is most available around blogs and youtube.\nTips: Ensure you understand the challenge before you start. I see so many folks jumping into solutions without really understanding the problem. Over communicate: share your your plan on how you will tackle the problem, and share your thought process as you go, including parts you have doubts on. Don\u0026rsquo;t treat tools as black boxes that just magically work. I lost count of times where I\u0026rsquo;m trying to have a candidate design a system with specific constraints of latency or scale, where the part of the challenge is assessing the right data structure, partitioning, indexing, carefully thinking Big O of each operation, defining what things might be read or write time computed, etc, and the candidate instead just throws a \u0026ldquo;I\u0026rsquo;ll just use Redis, it\u0026rsquo;s fast and should handle all that\u0026rdquo;. Offer Stage Before the market got tight, I would say to folks (not people I was interviewing, but people that asked me for advice) that once you get through all interviews you are most likely getting an offer. This is because the talent pool was smaller, so we rarely were at a point where we had multiple strong candidates that met our bar. At times, after exhaustive search, we had to lower the bar to fill a role. Today things are a bit different: you might kill it at all the interviews and still not get an offer because someone else was stronger. (More common for junior roles. Strong candidates especially in cloud, AI, data, etc. are still very competitive).\nIf you are not receiving timely response or offer after all the interviews, it\u0026rsquo;s likely one of the following reasons:\nthere\u0026rsquo;s a couple more candidates at the end of the pipeline as well that they want to wait before making a final decision your offer might be stuck in compensation discussions Once the hiring manager makes a decision they will recommend to the recruiter to offer you the position, and the recruiter will start working with compensation team to come up with an offer.\nDepending on how long the official offer takes, the recruiter might reach out and informally let you know that they expect to move forward with an offer, but it won\u0026rsquo;t give you the specifics yet. For them to get an official offer out they will need to work with a HR compensation team.\nThe hiring manager will provide some guidance on where they think this person would fit for the role: should they be at the lower-end, mid, or higher of compensation, or even if it should be considered a different level for this person. The recruiter will share this information with the compensation team, which will then assess this information alongside market rates, internal equity (what peers on that team are currently being compensated), and skills you have on paper: years of experience, degrees, previous companies, and sometimes what you answered about expected compensation. Once they have a compensation package, they will share it with the hiring manager for feedback. Depending on the candidate, the hiring manager might want to influence the offer in one direction or another, and it might take some back-and-forth until the hiring manager approves, and then the compensation is shared with the candidate. Most of the time here the hiring manager will be trying to advocate on behalf of the candidate to get the best offer possible, but sometimes they might be constrained by internal policies and compensation bands.\nOffer negotiation is an entire topic on its own, so my goal here is just to explain the process, but not focusing on negotiation tips (there\u0026rsquo;s a lot of content out there from folks that are much better than me at this).\nFinal Thoughts I see a lot of posts online sharing frustration with interview process, which I can understand. I’ve been on the other side of the table, and I know how frustrating it can be to go through a long process that resemble an obstacle course that isn\u0026rsquo;t really assessing what you will be doing on the job, only to be rejected at the end with little feedback or explanation.\nBut understanding the challenges from the hiring side can help you navigate the process better: Hiring is a time-consuming process, and the hiring manager and interviewers are likely juggling multiple roles at once. The hiring decision is a huge responsibility that needs to be done with imperfect information, and hiring managers are constrained by the process and policies of the company, which can be frustrating for both sides.\nI hope this post helps you understand the process better, and gives you some tips to improve your chances of getting hired. If you have any questions or feedback, feel free to reach out!\n"},{"section":"Blog","slug":"/blog/post-nursery/","title":"Starry Sky Space-Themed Nursery","description":"A DIY project to create a starry sky in a nursery","date":"January 10, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/nursery/7f2e5227-7717-4543-9433-f1b4a9b7f973_hu10504947911366819850.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/nursery\\/7f2e5227-7717-4543-9433-f1b4a9b7f973_hu10504947911366819850.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/posts/nursery/7f2e5227-7717-4543-9433-f1b4a9b7f973_hu8338818794949531772.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/posts\\/nursery\\/7f2e5227-7717-4543-9433-f1b4a9b7f973_hu8338818794949531772.webp';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Personal Projects","tags":"personal, diy","content":"On January 1st, New Year Day, my daughter Lua was born. Her name means \u0026ldquo;moon\u0026rdquo; in Portuguese, but even before we had picked the name and had decided to make the nursery space-themed.\nWe opted to try a fairly ambitious project (in terms of time and labor). The goal was to get fiber optic filaments and put them coming out of the attic and down through the ceiling. When all you see is the tiny little tips of the filament being illuminated at the other end by LED, it can look like a starry sky.\nThis is the result of the project. During the day, it keeps a warm, gold-like color, matching the other star-shaped lights in her room:\nAt night we have them twinkle in white, with very light luminosity. Unfortunately, it’s really hard to capture it on an iPhone camera due to the very low light, but this is a view of the camera looking straight at the ceiling from the floor:\nMove Insulation When I bought the equipment and materials I had a naive assumption that it would be easy enough to pass the filaments through the insulation, since it’s fairly soft and porous. However, I quickly learned that while that was true, the insulation would get in the way of visibility to understand what I was doing and make it extremely uncomfortable to work while in the attic. I decided that it would be best to just move it out of the way by relocating all the insulation above the nursery to other areas of the attic.\nI tried different approaches including trying to bag it, just trying to grab a clump of it with both hands, shovels, etc. None of those methods were efficient. It would take days to get it to work. After a bit, I decided to try an electric leaf blower. When working at maximum power it could make the insulation move. It was still not easy or quick: I had to blow each segment multiple times since every time I tried to blow insulation in a direction, about 25% would blow back into areas that I had already blown. It was also really uncomfortable to keep working there. The roof is not that tall, so being up in the attic and moving around with a leaf blower felt like doing a constant plank workout. To move all the insulation to the point where the drywall ceiling was clear to work took maybe 10 sessions of 15-30min (which was the maximum I could handle physically). I was also wearing 3M dust mask, eye goggles, long sleeves, and pants, which was not very comfortable.\nBefore moving insulation After moving insulation\nPutting Filaments Through I subdivided the filaments into smaller groups. This worked well, but I learned later that I should have better planned how I would distribute each group through the different areas. Once I started putting the filaments I realized that things were getting messy quickly. Sometimes I accidentally dropped the un-bundled groups I had just to have all the filaments spread all over. Trying to find their ends up there was a hard.\nDrilling The first method I tried was to try to drill a hole from the attic down. This worked, but I noticed that caused the paint to crack slightly around the hole, which made it more visible than I liked.\nI then experimented with drilling the hole from the bottom up. It had the benefit of leaving the least of a mark around the pain that drilling from down from the attic. The disadvantage is that I would have to hunt down the individual holes up there, with limited visibility, and perform aerobics.\nFilament Placement The first approach I tried was to just put the filament through, glue it, and then trim the tip near the ceiling so it was flush with it. This didn’t work well, most of the tools I tried (plier, scissors, razor blade) didn’t do a good job enough to keep things flush while also not damaging the ceiling.\nThe alternate method I developed instead was to put the filament just enough to get through the drywall, and have a spotter (which had already become a necessity at this point) in the room that pushed the filament back with a flat object so it didn’t go beyond the ceiling, making it perfectly flush to it.\nI was glad that I had created the subdivisions of cables and taped them together before I started the work, although I could have done a better job “piping” them first through the different areas of the attic and through the beams to keep things more organized.\nI learned that things quickly got pretty messy, as much as I tried to keep it tidy.\nThings got out of hand here.\nGluing Gluing was perhaps the thing that took the most to develop a good technique. I tried gluing from the room on the ceiling. That didn’t work at all, so I realized I would have to glue it up there in the attic.\nThe hardest thing about gluing was the temperature at which the glue operated. Turns out that applying the glue directly on top of the filament would melt and bend the filament. When that happened, the light would start exiting through the area where it melted instead of through the tip on the other side of the ceiling. I tried multiple methods: putting the filament and then dropping the glue, holding the filament straight so the hot glue doesn’t bend the filament, pressing the gun until a little bit of glue comes out of the tip, and using a fan to cool it before putting it on the filament. The method that worked best was putting the glue on the hole, using a fan to cool it slightly (enough to not melt the filament but still be soft enough), and then passing the filament through the glue drop into the hole. On the coldest days I was working there (around 50°f) I could put the glue on the hole, wait a few seconds, and then pass the filament through it, and that was enough to not melt the filament.\nThe glue helped hold things in place, but nothing too strong. A few got tangled on my arm or feet while moving in the attic, which made me have to go back and re-do a bunch of them.\nOverall Tips If you want to try a similar project, here are some of the tips from what I learned along the way:\nUse protective gear! Fiberglass can make your skin itchy, and irritate lungs and eyes. Bring a cushion for your knees Wait for cool weather if you can Have a wooden plank that you can lay between the beams, so you can also rest your body Try to create a good filament organization system. I subdivided them into groups and sent each group to a different side of the room, but in retrospect I would have spent more time planning a “distribution system” for them (ie.: taping groups to the wood, maybe using pipes, etc. Equipment fiber optic filaments LED lights or project safety equipment (M3 masks, safety goggles for dust) Glue gun Drill with a very thin bit Tape (for cable organization and holding in place) Even though this project was physically and strategically challenging and took 40+ hours to complete, the result was well worth it. I hope my daughter will be mesmerized by the twinkling stars in her room for as long as she occupies it and always enjoys staring up at them before drifting off to sleep.\n"}]